{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a3a87034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f83949ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the files you are given\n",
    "df_tr = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "70c03735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "      <th>LEN</th>\n",
       "      <th>YR</th>\n",
       "      <th>MON</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HR</th>\n",
       "      <th>WK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "      <td>330</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "      <td>270</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "      <td>960</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "      <td>630</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "      <td>420</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710665</th>\n",
       "      <td>1404171463620000698</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000698</td>\n",
       "      <td>1404171463</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612469,41.14602],[-8.612487,41.145993],[-...</td>\n",
       "      <td>465</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710666</th>\n",
       "      <td>1404171367620000670</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000670</td>\n",
       "      <td>1404171367</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.610138,41.140845],[-8.610174,41.140935],[...</td>\n",
       "      <td>435</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710667</th>\n",
       "      <td>1388745716620000264</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000264</td>\n",
       "      <td>1388745716</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710668</th>\n",
       "      <td>1404141826620000248</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20000248</td>\n",
       "      <td>1404141826</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.630712,41.154885],[-8.63073,41.154813],[-...</td>\n",
       "      <td>915</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710669</th>\n",
       "      <td>1404157147620000079</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20000079</td>\n",
       "      <td>1404157147</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.615538,41.140629],[-8.615421,41.140746],[...</td>\n",
       "      <td>390</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1710670 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0        1372636858620000589         C          NaN           NaN  20000589   \n",
       "1        1372637303620000596         B          NaN           7.0  20000596   \n",
       "2        1372636951620000320         C          NaN           NaN  20000320   \n",
       "3        1372636854620000520         C          NaN           NaN  20000520   \n",
       "4        1372637091620000337         C          NaN           NaN  20000337   \n",
       "...                      ...       ...          ...           ...       ...   \n",
       "1710665  1404171463620000698         C          NaN           NaN  20000698   \n",
       "1710666  1404171367620000670         C          NaN           NaN  20000670   \n",
       "1710667  1388745716620000264         C          NaN           NaN  20000264   \n",
       "1710668  1404141826620000248         B          NaN          12.0  20000248   \n",
       "1710669  1404157147620000079         B          NaN          34.0  20000079   \n",
       "\n",
       "          TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0        1372636858        A         False   \n",
       "1        1372637303        A         False   \n",
       "2        1372636951        A         False   \n",
       "3        1372636854        A         False   \n",
       "4        1372637091        A         False   \n",
       "...             ...      ...           ...   \n",
       "1710665  1404171463        A         False   \n",
       "1710666  1404171367        A         False   \n",
       "1710667  1388745716        A         False   \n",
       "1710668  1404141826        A         False   \n",
       "1710669  1404157147        A         False   \n",
       "\n",
       "                                                  POLYLINE  LEN    YR  MON  \\\n",
       "0        [[-8.618643,41.141412],[-8.618499,41.141376],[...  330  2013    7   \n",
       "1        [[-8.639847,41.159826],[-8.640351,41.159871],[...  270  2013    7   \n",
       "2        [[-8.612964,41.140359],[-8.613378,41.14035],[-...  960  2013    7   \n",
       "3        [[-8.574678,41.151951],[-8.574705,41.151942],[...  630  2013    7   \n",
       "4        [[-8.645994,41.18049],[-8.645949,41.180517],[-...  420  2013    7   \n",
       "...                                                    ...  ...   ...  ...   \n",
       "1710665  [[-8.612469,41.14602],[-8.612487,41.145993],[-...  465  2014    6   \n",
       "1710666  [[-8.610138,41.140845],[-8.610174,41.140935],[...  435  2014    6   \n",
       "1710667                                                 []    0  2014    1   \n",
       "1710668  [[-8.630712,41.154885],[-8.63073,41.154813],[-...  915  2014    6   \n",
       "1710669  [[-8.615538,41.140629],[-8.615421,41.140746],[...  390  2014    6   \n",
       "\n",
       "         DAY  HR  WK  \n",
       "0          1   0   0  \n",
       "1          1   0   0  \n",
       "2          1   0   0  \n",
       "3          1   0   0  \n",
       "4          1   0   0  \n",
       "...      ...  ..  ..  \n",
       "1710665   30  23   0  \n",
       "1710666   30  23   0  \n",
       "1710667    3  10   4  \n",
       "1710668   30  15   0  \n",
       "1710669   30  19   0  \n",
       "\n",
       "[1710670 rows x 15 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "582f5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "0416639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6cc17f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_tr[df_tr[\"MISSING_DATA\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cab_data = pd.read_csv(\"metaData_taxistandsID_name_GPSlocation.csv\")\n",
    "# cab_dict = {}\n",
    "# for i in range(1, 64):\n",
    "#     cab_dict[i] = (cab_data[cab_data[\"ID\"] == i].values[0][2], cab_data[cab_data[\"ID\"] == i].values[0][3])\n",
    "# lat_mean = np.mean([cab_dict[i][0] for i in range(1, 64)])\n",
    "# lat_std = np.std([cab_dict[i][0] for i in range(1, 64)])\n",
    "# long_mean = np.mean([cab_dict[i][1] for i in range(1, 64)])\n",
    "# long_std = np.std([cab_dict[i][1] for i in range(1, 64)])\n",
    "# for i in range(1, 64):\n",
    "#     old_lat = cab_dict[i][0]\n",
    "#     old_long = cab_dict[i][1]\n",
    "#     new_lat = (old_lat - lat_mean) / lat_std\n",
    "#     new_long = (old_long - long_mean) / long_std\n",
    "#     cab_dict[i] = (new_lat + 3, new_long + 3) # push up the z-normalized values so we can use 0 as a placeholder for \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f1e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cab_data = pd.read_csv(\"metaData_taxistandsID_name_GPSlocation.csv\")\n",
    "# cab_dict = {}\n",
    "# # for i in range(1, 64):\n",
    "#     cab_dict[i] = (cab_data[cab_data[\"ID\"] == i].values[0][2], cab_data[cab_data[\"ID\"] == i].values[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8400cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_lat_long(x):\n",
    "#     i = x[\"ORIGIN_STAND\"]\n",
    "# #     if (i != i): return 0, 0 # placeholder for null values\n",
    "#     i = int(i)\n",
    "#     return cab_dict[i][0], cab_dict[i][1]\n",
    "\n",
    "# df_tr[[\"LAT\", \"LONG\"]] = df_tr[[\"ORIGIN_STAND\"]].apply(get_lat_long, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c756f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "625ef1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "b5d56886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, target_transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.dataframe.iloc[idx]\n",
    "        time = torch.tensor([entry[\"LEN\"]]).to(torch.float32).to(device)\n",
    "        if (entry[\"ORIGIN_STAND\"] != entry[\"ORIGIN_STAND\"]): # if ORIGIN_STAND is NaN\n",
    "            # idea to do one-hot encoding comes from https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33\n",
    "            origin_stand = [0 for _ in range(63)]\n",
    "        else:\n",
    "            origin_stand = F.one_hot(torch.tensor(int(entry[\"ORIGIN_STAND\"]) - 1), num_classes=63).tolist()\n",
    "        feature_tuple = (entry[\"YR\"], entry[\"MON\"], entry[\"WK\"], entry[\"DAY\"], entry[\"HR\"], *origin_stand)\n",
    "        feature_tensor = torch.tensor(feature_tuple).to(torch.float32).to(device)\n",
    "        return feature_tensor, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "42de3e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710660"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ac2cf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://stackoverflow.com/questions/54730276/how-to-randomly-split-a-dataframe-into-several-smaller-dataframes\n",
    "shuffled = df_tr.sample(frac=1)\n",
    "result = np.array_split(shuffled, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "a511e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(result[:4])\n",
    "outlier_threshold = 3 # taking out anomalies has helped from my experience\n",
    "mean, std = train_df[\"LEN\"].mean(), train_df[\"LEN\"].std()\n",
    "train_df = train_df[train_df[\"LEN\"] < mean + outlier_threshold * std]\n",
    "test_df = result[-1]\n",
    "train_set = TaxiDataset(train_df)\n",
    "test_set = TaxiDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "ebcc74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "25d58bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8de73",
   "metadata": {},
   "source": [
    "Features are going to be:\n",
    "- Year\n",
    "- Month\n",
    "- Day\n",
    "- Hr\n",
    "- Week\n",
    "- One-hot encoding of taxi stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "0057a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(68, 75)\n",
    "        self.layer2 = nn.Linear(75, 75)\n",
    "        self.layer3 = nn.Linear(75, 75)\n",
    "        self.layer4 = nn.Linear(75, 75)\n",
    "        self.layer8 = nn.Linear(75, 1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(75)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(75)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(75) \n",
    "        self.batchnorm4 = nn.BatchNorm1d(75)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(self.relu(self.layer1(x)))\n",
    "        x = self.batchnorm2(self.relu(self.dropout((self.layer2(x)))))\n",
    "        x = self.batchnorm3(self.relu(self.layer3(x)))\n",
    "        x = self.batchnorm4(self.relu(self.dropout(self.layer4(x))))\n",
    "        x = self.relu(self.layer8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "6137c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i tried many designs, but next time i'm gonna use https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33\n",
    "# for inspiration, just make each layer 256 (and use lots of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "99f87c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    model.eval()\n",
    "\n",
    "def eval_train_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            train_loss += loss_fn(pred, y).item()\n",
    "    train_loss /= num_batches\n",
    "    print(f\"Avg train loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg test loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043277f3",
   "metadata": {},
   "source": [
    "The results below demonstrate why we chose learning rate of 1e-4 as opposed to the \"normal\" learning rate of 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "bed87b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg train loss: 635068.323427 \n",
      "\n",
      "Avg test loss: 928501.506996 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg train loss: 257798.895544 \n",
      "\n",
      "Avg test loss: 468819.074102 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Avg train loss: 4179242959083146838016.000000 \n",
      "\n",
      "Avg test loss: 3990624343876694441984.000000 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Avg train loss: 3762059.166132 \n",
      "\n",
      "Avg test loss: 9341374.018715 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Avg train loss: 175583.400580 \n",
      "\n",
      "Avg test loss: 411409.032795 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Avg train loss: 1446575487985.299561 \n",
      "\n",
      "Avg test loss: 1222745804070.749756 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Avg train loss: 190183.054724 \n",
      "\n",
      "Avg test loss: 439402.916744 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Avg train loss: 174695.300122 \n",
      "\n",
      "Avg test loss: 409606.637185 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Avg train loss: 207696.271067 \n",
      "\n",
      "Avg test loss: 425557.110687 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg train loss: 174431.850568 \n",
      "\n",
      "Avg test loss: 419362.896076 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "# code taken from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    eval_train_loop(train_loader, model, loss_fn)\n",
    "    test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "05713339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(weight_decay, momentum):\n",
    "    model = Net().to(device)\n",
    "    # code taken from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 64\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "    epochs = 10\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_loader, model, loss_fn, optimizer)\n",
    "        eval_train_loop(train_loader, model, loss_fn)\n",
    "        test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "35cc6c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg train loss: 176656.143390 \n",
      "\n",
      "Avg test loss: 412215.561013 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg train loss: 184245.964222 \n",
      "\n",
      "Avg test loss: 427208.596927 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Avg train loss: 178819.849611 \n",
      "\n",
      "Avg test loss: 412331.803871 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Avg train loss: 179098.322536 \n",
      "\n",
      "Avg test loss: 415937.672633 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Avg train loss: 174251.355406 \n",
      "\n",
      "Avg test loss: 409023.052589 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Avg train loss: 178918.936778 \n",
      "\n",
      "Avg test loss: 424530.758017 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Avg train loss: 174700.587944 \n",
      "\n",
      "Avg test loss: 414424.797341 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Avg train loss: 175454.213596 \n",
      "\n",
      "Avg test loss: 406181.459626 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Avg train loss: 177387.507153 \n",
      "\n",
      "Avg test loss: 415012.519094 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg train loss: 174423.861363 \n",
      "\n",
      "Avg test loss: 409317.401323 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests(0.0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "28788b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg train loss: 178638.407207 \n",
      "\n",
      "Avg test loss: 419055.116021 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg train loss: 184578.827389 \n",
      "\n",
      "Avg test loss: 426798.667940 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Avg train loss: 179000.291345 \n",
      "\n",
      "Avg test loss: 412743.100280 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Avg train loss: 206190.028827 \n",
      "\n",
      "Avg test loss: 454690.719129 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Avg train loss: 185151.356658 \n",
      "\n",
      "Avg test loss: 411653.638118 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Avg train loss: 175497.524535 \n",
      "\n",
      "Avg test loss: 408742.309527 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Avg train loss: 174955.176013 \n",
      "\n",
      "Avg test loss: 409204.617741 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Avg train loss: 177168.024616 \n",
      "\n",
      "Avg test loss: 415731.075297 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Avg train loss: 176564.044695 \n",
      "\n",
      "Avg test loss: 414135.902460 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg train loss: 174923.563704 \n",
      "\n",
      "Avg test loss: 410703.671671 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests(0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "95a6f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg train loss: 618861.370700 \n",
      "\n",
      "Avg test loss: 909625.138060 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg train loss: 196532.464384 \n",
      "\n",
      "Avg test loss: 433782.717817 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Avg train loss: 175405.290079 \n",
      "\n",
      "Avg test loss: 408307.798420 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Avg train loss: 179244.544614 \n",
      "\n",
      "Avg test loss: 419215.357130 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Avg train loss: 191556.296058 \n",
      "\n",
      "Avg test loss: 437562.401265 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Avg train loss: 176239.032097 \n",
      "\n",
      "Avg test loss: 410178.541715 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Avg train loss: 193317.999035 \n",
      "\n",
      "Avg test loss: 426334.022009 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Avg train loss: 175768.150595 \n",
      "\n",
      "Avg test loss: 413438.548682 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Avg train loss: 179736.115487 \n",
      "\n",
      "Avg test loss: 409613.419135 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg train loss: 176794.108278 \n",
      "\n",
      "Avg test loss: 414395.423770 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests(0.4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "33ab6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg train loss: 176888.582779 \n",
      "\n",
      "Avg test loss: 412963.938141 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg train loss: 181508.986267 \n",
      "\n",
      "Avg test loss: 419785.563316 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Avg train loss: 176554.760876 \n",
      "\n",
      "Avg test loss: 414742.072674 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Avg train loss: 180262.732553 \n",
      "\n",
      "Avg test loss: 419375.896718 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Avg train loss: 181527.859957 \n",
      "\n",
      "Avg test loss: 416136.754693 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Avg train loss: 303610.666927 \n",
      "\n",
      "Avg test loss: 554356.691873 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Avg train loss: 176042.509197 \n",
      "\n",
      "Avg test loss: 413806.807894 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Avg train loss: 194731.906964 \n",
      "\n",
      "Avg test loss: 441729.108326 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Avg train loss: 278638.100141 \n",
      "\n",
      "Avg test loss: 514510.511777 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg train loss: 179336.036912 \n",
      "\n",
      "Avg test loss: 417660.097919 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests(0.6, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "270bd3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://discuss.pytorch.org/t/how-to-create-mlp-model-with-arbitrary-number-of-hidden-layers/13124/5\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, hidden_cnt):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(68, 75))\n",
    "        for i in range(hidden_cnt):\n",
    "            self.layers.append(nn.Linear(75, 75))\n",
    "            self.layers.append(nn.LeakyReLU())\n",
    "            self.layers.append(nn.BatchNorm1d(75))\n",
    "        self.layers.append(nn.Linear(75, 1))\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer(input_data)\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "97c301b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_loop(dataloader, model):\n",
    "    results = []\n",
    "    for (x, _) in dataloader:\n",
    "        for singleton in model(x):\n",
    "            results.append(singleton.item())\n",
    "    print(f\"Standard Deviation: {np.std(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "f54a757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests_2(weight_decay, momentum, n_hidden_layers, learning_rate=1e-4, batch_size=64):\n",
    "    model = Net2(n_hidden_layers).to(device)\n",
    "    # code taken from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "    epochs = 10\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_loader, model, loss_fn, optimizer)\n",
    "        eval_train_loop(train_loader, model, loss_fn)\n",
    "        test_loop(test_loader, model, loss_fn)\n",
    "        std_loop(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "8569e1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Avg train loss: 608107220281.666382 \n",
      "\n",
      "Avg test loss: 608602295103.854126 \n",
      "\n",
      "Standard Deviation: 741932.8741829855\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Avg train loss: 17257327577893426954240.000000 \n",
      "\n",
      "Avg test loss: 17083278106008678825984.000000 \n",
      "\n",
      "Standard Deviation: 86877486317.58066\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_470/1387063563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_tests_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_470/2003703673.py\u001b[0m in \u001b[0;36mrun_tests_2\u001b[0;34m(weight_decay, momentum, n_hidden_layers, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0meval_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_470/3974635993.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_470/3955725857.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_tests_2(0.2, 0.3, 40, learning_rate=1e-5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "23ffc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensor(entry):    \n",
    "    if (entry[\"ORIGIN_STAND\"] != entry[\"ORIGIN_STAND\"]): # if ORIGIN_STAND is NaN\n",
    "        # idea to do one-hot encoding comes from https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33\n",
    "        origin_stand = [0 for _ in range(63)]\n",
    "    else:\n",
    "        origin_stand = F.one_hot(torch.tensor(int(entry[\"ORIGIN_STAND\"]) - 1), num_classes=63).tolist()\n",
    "    feature_tuple = (entry[\"YR\"], entry[\"MON\"], entry[\"WK\"], entry[\"DAY\"], entry[\"HR\"], *origin_stand)\n",
    "    feature_tensor = torch.tensor(feature_tuple).to(torch.float32).to(device)\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b0d7c",
   "metadata": {},
   "source": [
    "I ran into a problem where the model would just output the exact same value every time. I combatted this by adding more layers + batch normalization + reduced learning rate + less epochs.\n",
    "\n",
    "https://datascience.stackexchange.com/questions/58220/how-to-deal-with-a-constant-value-as-an-output-from-neural-network\n",
    "\n",
    "https://stackoverflow.com/questions/4493554/neural-network-always-produces-same-similar-outputs-for-any-input\n",
    "\n",
    "https://stackoverflow.com/questions/39217567/keras-neural-network-outputs-same-result-for-every-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f864b093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 8, 14, 17, 3)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = datetime.fromtimestamp(1408039037)\n",
    "dt.year, dt.month, dt.day, dt.hour, dt.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c5cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "240bb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_test = pd.read_csv(\"test_public.csv\")\n",
    "df_public_test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_public_test[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "pred_dict = {}\n",
    "for row in df_public_test.iloc:\n",
    "    pred_dict[row[\"TRIP_ID\"]] = model(torch.unsqueeze(get_input_tensor(row), dim=0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "5b13212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x):\n",
    "    return pred_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "b18820e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample submission file that is given on kaggle\n",
    "df_sample = pd.read_csv(\"sampleSubmission.csv\")\n",
    "df_sample[\"TRAVEL_TIME\"] = df_sample[\"TRIP_ID\"].apply(get_prediction)\n",
    "df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "864a464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinReg, self).__init__()\n",
    "        self.layer1 = nn.Linear(68, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "fc4a98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_model = LinReg().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linreg_model.parameters(), lr=learning_rate, weight_decay=0.99)\n",
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, linreg_model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, linreg_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
